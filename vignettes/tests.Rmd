---
title: "tests"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include = F}

devtools::document()
library(sl3)
```

```{r}
library(simcausal)
D <- DAG.empty()
  D <- D +
    node("W", distr = "runif", min = 0, max = 1.5) +
    node("A", distr = "rbinom", size = 1, prob = .15 + .5 * as.numeric(W > .75)) +
    node("Trexp", distr = "rweibull", shape = 1 + .5 * W, scale = 6) +
    node("Cweib", distr = "rweibull", shape = 1 + .5 * W, scale = 15) +
    node("T", distr = "rconst", const = min(ceiling(Trexp ), 12)) +
    node("C", distr = "rconst", const = ceiling(Cweib )) +
    # Observed random variable (follow-up time):
    node("T.tilde", distr = "rconst", const = ifelse(T <= C, T, C)) +
    # Observed random variable (censoring indicator, 1 - failure event, 0 - censored):
    node("Delta", distr = "rconst", const = ifelse(T <= C, 1, 0))
  setD <- set.DAG(D)
  dat <- sim(setD, n = 1000)
  # only grab ID, W's, A, T.tilde, Delta
  Wname <- grep("W", colnames(dat), value = TRUE)
  dat <- dat[, c("ID", Wname, "A", "T.tilde", "Delta")]
  dat
dat$id <- dat$ID
dat$ID <- NULL
dat$processN <- dat$Delta
dat$processA <- 1 - dat$Delta
dat$t <- dat$T.tilde
dat$T.tilde <- NULL

init_dat = copy(dat)
init_dat$t = 0
init_dat$processN <- 0
init_dat$processA <- 0
long_data <- data.table(rbind(init_dat, dat))
setkey(long_data, id, t)
long_data

```

```{r}
# at_risk indicators for degeneracy
at_risk_A <- function(X,time, ...){
  index_strict <- X$t < time
  as.numeric(all(X$processN==0) & all(X$processA[index_strict] ==0))
}
at_risk_N <- function(X, time, ...){
  index_strict <- X$t < time
  as.numeric(all(X$processN[index_strict]==0) & all(X$processA[index_strict] ==0))
}

long_data[t <= 10, at_risk_N(.SD, 10), by = id]

at_risk_A <- Summary_measure$new(c("processA", "processN", "t"), at_risk_A)
at_risk_N <- Summary_measure$new(c("processA", "processN", "t"), at_risk_N)

npsem = list(define_node("processN", "processN", c("W", "A"), time = sort(unique(setdiff(long_data$t, 0))), risk_set_map = at_risk_N, missing_row_implies_not_at_risk = F),
             define_node("processA", "processA", c("W", "A"), time = sort(unique(setdiff(long_data$t, 0))), risk_set_map = at_risk_A, missing_row_implies_not_at_risk = F),
             define_node("A", "A", c("W"), time = 0),
              define_node("W", "W", c(), time = 0))
task <- tmle3_Task$new(long_data, npsem, t= "t", id = "id")

factor_list = list(
  LF_emp$new("W"),
  LF_fit$new("A", make_learner(Lrnr_glm)),
    LF_fit$new("processA", make_learner(Lrnr_glm), type = "mean"),
      LF_fit$new("processN", make_learner(Lrnr_glm), type = "mean")
)
```



```{r}
task <- tmle3_Task$new(long_data, npsem, t= "t", id = "id")

lik <- Likelihood$new(factor_list)

lik <- lik$train(task)

cf_task <- task$clone()
cf_task$force_at_risk <- T


cf_task$get_regression_task("processN", expand = T)$get_data()
task$get_regression_task("processN", expand = T)$get_data()
task$get_tmle_node("processN", compute_risk_set = T)
```



```{r}
tlik <- Targeted_Likelihood$new(lik, updater = list(one_dimensional=T, maxit = 2, convergence_type = "sample_size"))
param_surv <- Param_survival$new(tlik, list("A" = LF_static$new("A", value = 1)), outcome_node = "processN")
tlik$updater$update(tlik, task)
#tlik$get_likelihood(cf_task, "processN")
```



```{r}

Gradient$new()
```


```{r}
task <- tmle3_Task$new(long_data, npsem, time = "t", id = "id")

# Check pooled regression tasks coincide (up to order of rows due to id and time)
pooled = task$get_regression_task(c("L1", "L2"), expand = T)$data
stacked = rbindlist(list(task$get_regression_task(c("L1"), expand = T, is_time_variant =  T)$data,
task$get_regression_task(c("L2"), expand = T, is_time_variant = T)$data), use.names = F)
setkey(stacked, id , t)
setkey(pooled, id , t)
assertthat::assert_that(all(stacked == pooled, use.names = T))


l1 <- lik$get_likelihood(task, "L1", drop_id = F, drop_time = T, to_wide = F)
l2 <- lik$get_likelihood(task, "L2", drop_id = F, drop_time = T, to_wide = F)
l12 <- lik$get_likelihood(task, c("L1", "L2"), drop_id = F, drop_time = F, to_wide = T)
merged_l12 = merge(l1, l2, by = c("id"))
# Check that pooled and unpooled predictions coincide
assertthat::assert_that(all(l12[, c("L1", "L2")] ==  merged_l12[, c("L1", "L2")]))
```

```{r}
lik$get_likelihood(task, "L1", drop_id = F, drop_time = F)
```

```{r}
task$data
generator <-function(task, Likelihood = NULL, target_param = NULL, node, outcome = T){
  task <- task$get_regression_task(node)
  cols <- task$add_columns(data.table(IC = rnorm(task$nrow)))
  task$next_in_chain(column_names = cols, covariates  = c(task$nodes$covariates, task$nodes$outcome), outcome = "IC")
}
generator(task, node = "Y")$data
```

```{r}
library(data.table)

# Generate simple causal model
library(simcausal)

D <- DAG.empty()
D <- D +
  node("W", distr = "runif", min =-1, max = 1) +
  node("A", distr = "rbinom", size = 1, prob = 0.5 + W/4 ) +
  node("T", distr = "rexp", rate = 0.05*(1 + 0.5*A - 0.3*W) )+
  node("C", distr = "rexp",  rate = 0.03*(1 + 0.4*A - 0.2*W)) +
  node("Delta", distr = "rconst", const = as.numeric(T<=C)  ) +
  node("Ttilde", distr = "rconst", const = round(min(T,C,10) +1  ))
setD <- set.DAG(D)
dat <- sim(setD, n = 1e2)
# only grab ID, W's, A, T.tilde, Delta
Lname <- grep("W", colnames(dat), value = TRUE)
Aname <- grep("A", colnames(dat), value = TRUE)
df <- dat[, c("ID", Lname, Aname, "T", "C", "Delta", "Ttilde")]

data <- df[,c( "W", "A", "Ttilde", "Delta")]
data$id = df$ID

data$t <- data$Ttilde 
data = data.table(data)
data$processA = as.numeric(data$Delta ==0)
data$processN = as.numeric(data$Delta ==1)
data_baseline <- data
data_baseline$t =0
data_baseline$processA = 0
data_baseline$processN = 0
data_baseline$Ttilde = NA
data_baseline$Delta = NA
data = rbind(data_baseline, data)
data$Ttilde = NULL
data$Delta = NULL
data$at_risk = 1 - (data$processA + data$processN)
data[which(data$processN==1), "processA"] = 0
npsem = list(define_node("W", c("W"), time =0), 
     define_node("A", c("A"), c("W"), time =0, summary_functions = make_summary_measure_baseline(c("W"))),
     
     define_node("processA", c("processA"),  c("A","W"), time = seq(1,max(data$t)), summary_functions = make_summary_measure_baseline(c("A", "W")),
                  risk_set_map = make_summary_measure_last_value("at_risk", strict_past  = T), missing_row_implies_not_at_risk = F),
     
define_node("processN", c("processN"), c("A","W"),time = seq(1,max(data$t)), summary_functions = make_summary_measure_baseline(c("A", "W")),
                  risk_set_map = make_summary_measure_last_value("at_risk", strict_past = T), missing_row_implies_not_at_risk = F))
# Delta is the counting process which is 1 at death
data
```


 
```{r}
c(T,T, F) && T
```

```{r}
library(sl3)
devtools::document()

```


```{r}
n=500
way = data.table(W = (rbinom(n, size = 50, 0.5)), A = (rbinom(n, size = 50, 0.5)), Y = (rbinom(n, size = 300, 0.5)))
way
generator <-function(task, Likelihood = NULL, target_param = NULL, node, outcome = T){
  task <- task$get_regression_task(node)
  cols <- task$add_columns(data.table(Y= as.numeric(as.character(task$Y)) , IC = as.numeric(as.character(task$Y)) + rowSums(task$X)))
  task$next_in_chain( column_names = cols, covariates  = c(task$nodes$covariates, task$nodes$outcome), outcome = "IC")
}

library(data.table)
npsem = list(define_node("W", c("W")), define_node("Y", c("Y"), c("A", "W"), variable_type = variable_type(type = "categorical", levels = sort(unique(way$Y)))),  define_node("A", c("A"), "W"))


task <- tmle3_Task$new(as.data.table(way), npsem)
way
task$get_regression_task("Y")$data
task$npsem$Y$variable_type$type
library(sl3)
likelihood_factors <- list("W" = LF_emp$new("W" ), "A" = LF_fit$new("A", make_learner(Lrnr_condensier)), "Y" =  LF_fit$new("Y", make_learner(Lrnr_pooled_hazards), type = "density"))
liks <- Likelihood$new(likelihood_factors)


liks = liks$train(task)
grad <- Gradient$new(liks, generator, list(update_nodes = "Y"))

grad <- grad$train(task)
grad$compute_component(task, "Y")

```

```{r}
setattr(task, "target_nodes", "Y")
    AA~e1d  fqfv`deGBBG

```
```{r}
levels = sort(unique(way$trueY))

way$trueid <- seq_len(500)
way$trueY <- way$Y
w=rbindlist(lapply(levels, function(level) {
  way <- copy(way)
  set(way ,, "Y", level)
  way
}))
w$id <- paste(w$trueid, w$Y, sep = "_")
w
expand_task = function(tmle_task, node){
  variables <- tmle_task$npsem[[node]]$variables
  if(length(variables) >1) stop("Multivariate nodes not supported")
  data <- tmle_task$data
  data$trueid <- data$id
  levels <- sort(unique(unlist(data[, variables, with = F])))
  long_data <- rbindlist(lapply(levels, function(level) {
    data <- copy(data)
    set(data ,, variables, level)
    return(data)
  }))
  long_data$id <-  paste(long_data$trueid,long_data[, variables, with = F][[1]], sep = "_")
  long_task <- tmle3_Task$new(long_data, tmle_task$npsem, id = "id", time = "t", force_at_risk = tmle_task$force_at_risk, summary_measure_columns = c(tmle_task$summary_measure_columns, "trueid"))
  return(long_task)
}
expand_task(task, "Y")$data
```

```{r}
task$force_at_risk
```
```{r}
grad <- Gradient$new(liks, generator, list(update_nodes = "Y"))

grad <- grad$train(task)
out = (grad$compute_component(task, "Y"))
lapply(out, as.data.table)
```



```{r}

preds <- liks$factor_list$Y$get_density(task, "full", quick_pred = T)
levels <- task$npsem$Y$variable_type$levels
preds <- data.table(sl3::unpack_predictions(as.vector(preds)))
preds
setnames(preds, as.character(seq_along(levels)))
preds
cdf <- data.table(t(apply(preds, 1, cumsum)))
X = as.matrix(cbind(task$get_regression_task("Y")$X, as.numeric(as.character(task$get_regression_task("Y")$Y))))
basis_list = hal9001::enumerate_basis(X, 1)
data.table(X)
design1 <- as.data.table(as.matrix(hal9001::make_design_matrix(X, basis_list)))
```




```{r}
design <- copy(design1)
# Design matrix where Y part of indicator always evaluated to 1
clean_basis <- function(basis){
  if(!(3 %in% basis$cols)){
    return(basis)
  }
  index = which(basis$cols == 3)
  basis$cutoffs[index] <- -100
  return(basis)
}
clean_list = lapply(basis_list, clean_basis)
clean_design <- hal9001::make_design_matrix(X, clean_list)
clean_design <- data.table(as.matrix(clean_design))



# Get basis functions to change and their cdf column
diff_map <- unlist(lapply(seq_along(basis_list), function(i) {
  basis <- basis_list[[i]]
  if(!(3 %in% basis$cols)){
    return(NULL)
  }
  result <- (list(which(levels == basis$cutoffs[which(basis$cols == 3)])))
  names(result) = i
  return(result)
}))

donothing <- lapply((names(diff_map)), function(i){
  col_index <- diff_map[[i]]
  diff <- design[[as.integer(i)]] - 1 + cdf[[col_index]] 
  set(design, , as.integer(i), diff)
})

result = (design* clean_design)
dim(result)
```



```{r}
grad <- Gradient$new(liks, generator, list(update_nodes = "Y"))

grad <- grad$train(task)
grad$learner
grad$compute_component(task, "Y")
```

```{r}
grad$compute_component(task, "Y")

```

```{r}
n = 5
task[n]$data
i = 23
grad$fit_object$Y$basis_list[[i]]
b = grad$basis[["Y"]][[i]]
b(task[n], "full")
```







